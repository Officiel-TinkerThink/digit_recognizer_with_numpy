{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(686)\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# ===== Data Load and Prep =======\n",
    "# ================================\n",
    "\n",
    "\"\"\" prep binary classification data for mnist 4s vs 9s\"\"\"\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "dataset = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIG_A, DIG_B = 4, 9\n",
    "SIDE = 28\n",
    "MAX_PIX_VAL = 255\n",
    "NB_TRAIN = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# x_train is N_ x SIDE x SIDE array of integers [0,255]\n",
    "# x_train is N_  array of integers [0,9]\n",
    "# where N_ = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== keep only two digits of interest ==================\n",
    "def filter_digits(xs,ys):\n",
    "    indices = np.logical_or(\n",
    "        np.equal(ys, DIG_A),\n",
    "        np.equal(ys, DIG_B))\n",
    "\n",
    "    xs = xs[indices]\n",
    "    ys = ys[indices]\n",
    "    return xs, ys\n",
    "\n",
    "x_train, y_train = filter_digits(x_train, y_train)\n",
    "x_test, y_test = filter_digits(x_test, y_test)\n",
    "\n",
    "# x_train is N x SIDE x SIDE array of integers [0,255]\n",
    "# x_train is N  array of integers [4,9]\n",
    "# where N ~ 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11791"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== shape ==========================================\n",
    "assert len(x_train) == len(y_train)\n",
    "N = len(x_train)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= normalize pixel intensities ======================\n",
    "x_train = x_train / float(MAX_PIX_VAL)\n",
    "x_test = x_test / float(MAX_PIX_VAL)\n",
    "\n",
    "# x_train is N x SIDE x SIDE array of integers [0., 1.]\n",
    "# x_train is N  array of integers [4,9]\n",
    "# where N ~ 12000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= shuffle data ======================================\n",
    "def shuffle(xs, ys):\n",
    "    indices = np.arange(len(xs))\n",
    "    np.random.shuffle(indices) # mutating shuffle\n",
    "    xs = xs[indices]\n",
    "    ys = ys[indices]\n",
    "    return xs, ys\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "x_test, y_test = shuffle(x_test, y_test)\n",
    "\n",
    "\n",
    "# ======= pare down training set ============================\n",
    "x_train = x_train[:NB_TRAIN]\n",
    "y_train = y_train[:NB_TRAIN]\n",
    "\n",
    "# ======= add noise to pixel intensities ===============================\n",
    "x_train = x_train + np.random.randn(*x_train.shape)\n",
    "x_train = np.maximum(0., np.minimum(1., x_train))\n",
    "\n",
    "# ===== shape ==========================================\n",
    "assert len(x_train) == len(y_train)\n",
    "N = len(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Measure proximity\n",
    "# close_enough = lambda a, b : abs(b-a) < 1e-6\n",
    "close_enough = lambda a, b : np.linalg.norm((b - a).flatten()) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hooray!\n",
      "prepped 1000 training examples\n"
     ]
    }
   ],
   "source": [
    "# ======= Sanity Checks =====================================\n",
    "assert x_train.shape == (N, SIDE, SIDE)\n",
    "assert y_train.shape == (N,)\n",
    "assert set(y_train) == {DIG_A, DIG_B}\n",
    "assert close_enough(np.min(x_train), 0.)\n",
    "assert close_enough(np.max(x_train), 1.)\n",
    "assert abs(N-min(NB_TRAIN, 12000)) < 500\n",
    "\n",
    "print(\"hooray!\")\n",
    "print(\"prepped {} training examples\".format(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Metric for Success =========================================\n",
    "\n",
    "\"\"\" acc, loss \n",
    "\n",
    "    prob p represent model's prob mass for DIG_B\n",
    "    by 'predictor' I mean a function that takes in an image and gives a prob\n",
    "\"\"\"\n",
    "\n",
    "def accuracy(predicted_labels, true_ys):\n",
    "    return np.mean([1. if l==y else 0.\n",
    "                    for l, y in zip(predicted_labels, true_ys)])\n",
    "    \n",
    "def cross_entropy_loss(predicted_probs, true_ys):\n",
    "    return np.mean([ - np.log(p if y==DIG_B else 1.-p) \n",
    "                    for p,y in zip(predicted_probs, true_ys)])\n",
    "\n",
    "def judge(predictor, xs, ys, verbose=False):\n",
    "    xs = tqdm.tqdm(xs) if verbose else xs\n",
    "    probs = [predictor(x) for x in xs]\n",
    "    labels = [DIG_B if p > 0.5 else DIG_A for p in probs]\n",
    "    acc = accuracy(labels, ys)\n",
    "    loss = cross_entropy_loss(probs, ys)\n",
    "    return {\"acc\": acc, \"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599454 0.7184237591173707 0.7086925965227748\n",
      "hooray!\n"
     ]
    }
   ],
   "source": [
    "# ======= sanity checks using placeholder =========================================\n",
    "very_sure_A = lambda x : .01\n",
    "very_sure_B = lambda x : .99\n",
    "maybe_its_A = lambda x : .4\n",
    "maybe_its_B = lambda x : .6\n",
    "fifty_fifty = lambda x : .5\n",
    "\n",
    "vsa = judge(very_sure_A, x_train, y_train)[\"acc\"]\n",
    "vsb = judge(very_sure_B, x_train, y_train)[\"acc\"]\n",
    "assert close_enough(vsa + vsb, 1.)\n",
    "\n",
    "vsa = judge(very_sure_A, x_train[:1], [DIG_A])[\"acc\"]\n",
    "vsb = judge(very_sure_A, x_train[:1], [DIG_B])[\"acc\"]\n",
    "assert close_enough(vsa, 1.)\n",
    "assert close_enough(vsb, 0.)\n",
    "\n",
    "\n",
    "vsa = judge(very_sure_A, x_train, y_train)[\"loss\"]\n",
    "vsb = judge(very_sure_B, x_train, y_train)[\"loss\"]\n",
    "mia = judge(maybe_its_A, x_train, y_train)[\"loss\"]\n",
    "mib = judge(maybe_its_B, x_train, y_train)[\"loss\"]\n",
    "ffl = judge(fifty_fifty, x_train, y_train)[\"loss\"]\n",
    "assert ffl < mia < vsa\n",
    "assert ffl < mib < vsb\n",
    "assert close_enough(ffl, np.log(2))\n",
    "\n",
    "print(\"hooray!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# ====== LINEAR MODEL =======================================================\n",
    "# ==========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================    \n",
    "# ====== Manipulate Weights ======================================================\n",
    "linear_init = lambda : np.random.randn(SIDE*SIDE) / np.sqrt(SIDE*SIDE)\n",
    "\n",
    "def linear_displace(w, coef, g):\n",
    "    return w + coef * g\n",
    "\n",
    "# ===========================================================================    \n",
    "# ====== Forward Model ======================================================\n",
    "\n",
    "clip = lambda z : np.maximum(-15., np.minimum(+15., z))\n",
    "sigmoid = lambda z : 1./(1 + np.exp(-clip(z)))\n",
    "\n",
    "def linear_predict(w, x):\n",
    "    return sigmoid(w.dot(x.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hooray!\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "\n",
    "w = linear_init()\n",
    "\n",
    "vsa = judge(lambda x : linear_predict(+w, x), x_train, y_train)[\"acc\"]\n",
    "vsb = judge(lambda x : linear_predict(-w, x), x_train, y_train)[\"acc\"]\n",
    "assert close_enough(vsa + vsb, 1.)\n",
    "\n",
    "ffl = judge(lambda x: linear_predict(0*w, x), x_train, y_train)[\"loss\"]\n",
    "assert close_enough(ffl, np.log(2))\n",
    "\n",
    "x = w.reshape(SIDE, SIDE)\n",
    "vsa = judge(lambda x: linear_predict(w, x), [x], [DIG_A])[\"acc\"]\n",
    "vsb = judge(lambda x: linear_predict(w, x), [x], [DIG_B])[\"acc\"]\n",
    "assert close_enough(vsa, 0.)\n",
    "assert close_enough(vsb, 1.)\n",
    "\n",
    "print(\"hooray!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "# ======= Backward pass ==================================================\n",
    "\n",
    "\"\"\" For given x, y, we want derivative (with respect w) of\n",
    "    l(w) = loss(sigmoid(w.dot(x)), y)\n",
    "         = loss_at_y(sigmoid(dot_with_x(w))\n",
    "    where loss_at_y(p) = -log(p if y == DIG_B else 1-p)\n",
    "    where sigmoid(z) = 1/(1+exp(-z))\n",
    "    where dot_with_x(w) = w.dot(x)\n",
    "\n",
    "By CHAIN RULE : \n",
    "    l'(w) = (\n",
    "          loss_at_y'(sigmoid(dot_with_x(w)))\n",
    "        * sigmoid'(dot_with_x(w))\n",
    "        * dot_with_x'(w)\n",
    "    ) = (\n",
    "          loss_at_y'(p)\n",
    "        * sigmoid'(z)\n",
    "        * dot_with_x'(w)\n",
    "    )\n",
    "    where z = dot_with_x(w)\n",
    "    where p = sigmoid(z)\n",
    "    NOTE: appearance of terms from forward pass!\n",
    "\"\"\"\n",
    "\n",
    "def linear_backprop_unsimp(w, x, y):\n",
    "    z = w.dot(x.flatten())\n",
    "    p = sigmoid(z)\n",
    "    # this is correct ...\n",
    "    dl_dp = - (+1 if y==DIG_B else -1)/(p if y==DIG_B else 1-p)\n",
    "    dp_dz = p * (1 - p)\n",
    "    dz_dw = x.flatten()\n",
    "    #\n",
    "    dl_dw = dl_dp * dp_dz * dz_dw\n",
    "    return dl_dw\n",
    "\n",
    "def linear_backprop(w, x, y):\n",
    "    z = w.dot(x.flatten())\n",
    "    p = sigmoid(z)\n",
    "    # ..... and so is this\n",
    "    \"\"\"\n",
    "    dl_dp = -1/p if y==DIG_B else 1/(1-p)\n",
    "    dp_dz = p * (1 - p)\n",
    "    \"\"\"\n",
    "    # interpret dl_dz as error of p as estimator of one-hot version of y\n",
    "    dl_dz = p - (1 if y==DIG_B else 0)\n",
    "    dz_dw = x.flatten()\n",
    "    #\n",
    "    dl_dw = dl_dz * dz_dw\n",
    "    return dl_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hooray!\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "for _ in range(10):\n",
    "    w = linear_init()\n",
    "    idx = np.random.randint(N)\n",
    "    x = x_train[idx]\n",
    "    y = y_train[idx]\n",
    "\n",
    "    # check that simplification preserved answer\n",
    "    g_unsimp = linear_backprop_unsimp(w, x, y)\n",
    "    g              = linear_backprop(w, x, y)\n",
    "    assert close_enough(g_unsimp, g)\n",
    "\n",
    "    # do a step of gradient descent, check loss decreased\n",
    "    before = judge(lambda xx : linear_predict(w, xx), [x], [y])[\"loss\"]\n",
    "    w = linear_displace(w, -0.01, g)\n",
    "    after = judge(lambda xx: linear_predict(w, xx), [x], [y])[\"loss\"]\n",
    "    assert after < before\n",
    "\n",
    "print(\"hooray!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# ====== VANILLA MODEL ======================================================\n",
    "# ==========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================    \n",
    "# ====== Weight Helpers =====================================================\n",
    "\n",
    "D0 = SIDE*SIDE\n",
    "D1 = 32\n",
    "D2 = 32\n",
    "D3 = 1\n",
    "\n",
    "def vanilla_init():\n",
    "    A = np.random.randn(D3, D2) / np.sqrt( 1 + D2)\n",
    "    B = np.random.randn(D2, D1) / np.sqrt(D2 + D1)\n",
    "    C = np.random.randn(D1, D0) / np.sqrt(D1 + D0)\n",
    "    return (A,B,C)\n",
    "\n",
    "def vanilla_displace(abc, coef, g):\n",
    "    A, B, C = abc\n",
    "    gA, gB, gC = g\n",
    "    return (A + coef * gA,\n",
    "            B + coef * gB,\n",
    "            C + coef * gC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" what architecture? well, let's use this one:\\n    \\n    lrelu(z) = max(z/10, z)\\n    \\n    x\\n   h0 ---------> z1 -----> h1 ---------> z2 ----> h2 --------> z3 ------> p\\n    |\\n    |             |         |                                               \\n    |             |         |             |        |                        \\n    |     C*      | lrelu   |      B*     | lrelu  |     A*    | sigmoid  |\\n    |             |         |             |        |                      \\n    |             |         |                      1                      \\n    |                       1\\n    1                                                                   \\n    D0            D1        D1            D2       D2          1          1\\n    SIDE*SIDE     32                      32                   1          1\\n    \\n    D1 = 32; D2 = 32            \\n    \""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" what architecture? well, let's use this one:\n",
    "    \n",
    "    lrelu(z) = max(z/10, z)\n",
    "    \n",
    "    x\n",
    "   h0 ---------> z1 -----> h1 ---------> z2 ----> h2 --------> z3 ------> p\n",
    "    |\n",
    "    |             |         |                                               \n",
    "    |             |         |             |        |                        \n",
    "    |     C*      | lrelu   |      B*     | lrelu  |     A*    | sigmoid  |\n",
    "    |             |         |             |        |                      \n",
    "    |             |         |                      1                      \n",
    "    |                       1\n",
    "    1                                                                   \n",
    "    D0            D1        D1            D2       D2          1          1\n",
    "    SIDE*SIDE     32                      32                   1          1\n",
    "    \n",
    "    D1 = 32; D2 = 32            \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================    \n",
    "# ====== Forward Pass ======================================================\n",
    "\n",
    "lrelu     = lambda z : np.maximum(z/10, z)\n",
    "step      = lambda z : np.heaviside(z, 0.5)\n",
    "dlrelu_dz = lambda z: .1 + (1.-.1)*step(z)\n",
    "\n",
    "def vanilla_predict(abc, x):\n",
    "    A, B, C = abc\n",
    "    \n",
    "    h0 = x.flatten()\n",
    "    #\n",
    "    z1 = C.dot(h0)\n",
    "    h1 = lrelu(z1)\n",
    "    #\n",
    "    z2 = B.dot(h1)\n",
    "    h2 = lrelu(z2)  # this is our learned featurization\n",
    "    #\n",
    "    z3 = A.dot(h2) # linear classifier!\n",
    "    p = sigmoid(z3)\n",
    "    #\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hooray!\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "\n",
    "A,B,C = vanilla_init()\n",
    "\n",
    "# check that linear layer makes sense:\n",
    "vsa = judge(lambda x : vanilla_predict((+A,B,C), x), x_train, y_train)[\"acc\"]\n",
    "vsb = judge(lambda x : vanilla_predict((-A,B,C), x), x_train, y_train)[\"acc\"]\n",
    "assert close_enough(vsa + vsb, 1.)\n",
    "\n",
    "ffl = judge(lambda x: vanilla_predict((0*A,B,C), x), x_train, y_train)[\"loss\"]\n",
    "assert close_enough(ffl, np.log(2))\n",
    "\n",
    "# check end-to-end positivity\n",
    "x = x_train[0]\n",
    "y = y_train[0]\n",
    "A = np.abs(A)\n",
    "B = np.abs(B)\n",
    "C = np.abs(C)\n",
    "acc_ppp = judge(lambda x: vanilla_predict((A,B,C), x), [x], [DIG_B])[\"acc\"]\n",
    "acc_ppn = judge(lambda x: vanilla_predict((A,B,-C), x), [x], [DIG_B])[\"acc\"]\n",
    "acc_pnp = judge(lambda x: vanilla_predict((A,-B,C), x), [x], [DIG_B])[\"acc\"]\n",
    "acc_pnn = judge(lambda x: vanilla_predict((A,-B,-C), x), [x], [DIG_B])[\"acc\"]\n",
    "assert close_enough(acc_ppp, 1.)\n",
    "assert close_enough(acc_ppn, 0.)\n",
    "assert close_enough(acc_pnp, 0.)\n",
    "assert close_enough(acc_pnn, 1.)\n",
    "\n",
    "print(\"hooray!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_backprop(abc, x, y):\n",
    "    A, B, C = abc\n",
    "    \n",
    "    h0 = x.flatten()\n",
    "    #\n",
    "    z1 = C.dot(h0)\n",
    "    h1 = lrelu(z1)\n",
    "    #\n",
    "    z2 = B.dot(h1)\n",
    "    h2 = lrelu(z2)  # this is our learned featurization\n",
    "    #\n",
    "    z3 = A.dot(h2) # linear classifier!\n",
    "    p = sigmoid(z3)\n",
    "\n",
    "    dl_dz3 = p - (1 if y==DIG_B else 0)\n",
    "    dl_dh2 = dl_dz3 * A\n",
    "    dl_dz2 = dl_dh2 * dlrelu_dz(z2)\n",
    "    dl_dh1 = np.matmul(dl_dz2,B)\n",
    "    dl_dz1 = dl_dh1 * dlrelu_dz(z1)\n",
    "    # ATTN : figure out what \"*\" ought to mean\n",
    "\n",
    "    dl_dA = dl_dz3 * h2\n",
    "    dl_dB = np.outer(dl_dz2, h1)\n",
    "    dl_dC = np.outer(dl_dz1, h0)\n",
    "    \n",
    "    return (dl_dA, dl_dB, dl_dC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hooray!\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "    \n",
    "for _ in range(10):\n",
    "    abc = vanilla_init()\n",
    "    idx = np.random.randint(N)\n",
    "    x = x_train[idx]\n",
    "    y = y_train[idx]\n",
    "\n",
    "    # do a step of gradient descent, check loss decreased\n",
    "    before = judge(lambda xx : vanilla_predict(abc, xx), [x], [y])[\"loss\"]\n",
    "    g = vanilla_backprop(abc, x, y)\n",
    "    abc = vanilla_displace(abc, -.01, g)\n",
    "    after = judge(lambda xx: vanilla_predict(abc, xx), [x], [y])[\"loss\"]\n",
    "    assert after < before\n",
    "\n",
    "print(\"hooray!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# ====== CONVOLUTIONAL NEURAL NETWORK =======================================\n",
    "# ==========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================    \n",
    "# ====== Weight Helpers =====================================================\n",
    "\n",
    "D0 = SIDE*SIDE\n",
    "D1 = 32\n",
    "D2 = 32\n",
    "D3 = 1\n",
    "\n",
    "def conv_init():\n",
    "    A = np.random.randn(    5*5*4) / np.sqrt( 1 + 5*5*4)\n",
    "    B = np.random.randn(1,1,4,8) / np.sqrt(4 + 1*1*8)\n",
    "    C = np.random.randn(5,5,8,1) / np.sqrt(8 + 5*5*1)\n",
    "    return (A,B,C)\n",
    "\n",
    "def conv_displace(abc, coef, g):\n",
    "    A, B, C = abc\n",
    "    gA, gB, gC = g\n",
    "    return (A + coef * gA,\n",
    "            B + coef * gB,\n",
    "            C + coef * gC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' architecture\\n\\nin the chart below, we transform inputs (top) to outputs (bottom)\\n                height x width x channels\\n    x               28 x 28 x 1\\n        avgpool                                        2x2\\n    h0              14 x 14 x 1       \\n        conv                         weight C          5x5x8x1      stride 2x2\\n    z1              10 x 10 x 8\\n        lrelu\\n    h1              5  x 5  x 8       \\n        conv                         weight B          1x1x4x8      stride 1x1\\n    z2              5  x 5  x 4\\n        lrelu\\n    h2              5  x 5  x 4\\n        dense                        weight A          1x(5*5*4)\\n    z3                       32\\n        sigmoid\\n    p\\n\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" architecture\n",
    "\n",
    "in the chart below, we transform inputs (top) to outputs (bottom)\n",
    "                height x width x channels\n",
    "    x               28 x 28 x 1\n",
    "        avgpool                                        2x2\n",
    "    h0              14 x 14 x 1       \n",
    "        conv                         weight C          5x5x8x1      stride 2x2\n",
    "    z1              10 x 10 x 8\n",
    "        lrelu\n",
    "    h1              5  x 5  x 8       \n",
    "        conv                         weight B          1x1x4x8      stride 1x1\n",
    "    z2              5  x 5  x 4\n",
    "        lrelu\n",
    "    h2              5  x 5  x 4\n",
    "        dense                        weight A          1x(5*5*4)\n",
    "    z3                       32\n",
    "        sigmoid\n",
    "    p\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# ====== Building Blocks ====================================================\n",
    "\n",
    "def avgpool2x2(x):\n",
    "    H, W, C = x.shape\n",
    "    # return an array of shape (H/2 x W/2 x C)\n",
    "    return (x[0:H:2, 0:W:2] +\n",
    "            x[0:H:2, 1:W:2] +\n",
    "            x[1:H:2, 0:W:2] +\n",
    "            x[1:H:2, 1:W:2] )/4\n",
    "\n",
    "def conv(x, weights, stride=1):\n",
    "    H, W, C = x.shape\n",
    "    KH, KW, OD, ID = weights.shape\n",
    "    assert C==ID\n",
    "    HH, WW = int((H-KH+1)/stride), int((W-KW+1)/stride)\n",
    "    # return an array of shape HH x WW x OD\n",
    "    return np.array(\n",
    "        [[\n",
    "            np.tensordot(\n",
    "             weights          ,            # KH x KW x OD x ID \n",
    "             x[h:h+KH, w:w+KW],            # KH x KW      x ID\n",
    "             ((0,1,3), (0,1,2))\n",
    "            )\n",
    "          for w in range(0,WW*stride,stride)]\n",
    "         for h in range(0,HH*stride,stride)]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# ====== Forward Pass =======================================================\n",
    "\n",
    "\"\"\" architecture\n",
    "\n",
    "in the chart below, we transform inputs (top) to outputs (bottom)\n",
    "                height x width x channels\n",
    "    x               28 x 28 x 1\n",
    "        avgpool                                        2x2\n",
    "    h0              14 x 14 x 1       \n",
    "        conv                         weight C          5x5x8x1      stride 2x2\n",
    "    z1              10 x 10 x 8\n",
    "        lrelu\n",
    "    h1              5  x 5  x 8       \n",
    "        conv                         weight B          1x1x4x8      stride 1x1\n",
    "    z2              5  x 5  x 4\n",
    "        lrelu\n",
    "    h2              5  x 5  x 4\n",
    "        dense                        weight A          1x(5*5*4)\n",
    "    z3                       32\n",
    "        sigmoid\n",
    "    p\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def conv_predict(abc, x):\n",
    "    A, B,C = abc\n",
    "\n",
    "    h0 = avgpool2x2(x[:,:,np.newaxis])\n",
    "    #\n",
    "    z1 = conv(h0, C, stride=2)\n",
    "    #\n",
    "    h1 = lrelu(z1)\n",
    "    #\n",
    "    z2 = conv(h1, B, stride=1)\n",
    "    #\n",
    "    h2 = lrelu(z2)\n",
    "    #\n",
    "    z3 = A.dot(h2.flatten())\n",
    "    #\n",
    "    p = sigmoid(z3)\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hooray!\n"
     ]
    }
   ],
   "source": [
    "# sanity checks\n",
    "# scaling and shape tests\n",
    "aa = np.ones((8,12,7))\n",
    "pp = 1*np.ones((4,6,7))\n",
    "assert close_enough(avgpool2x2(aa), pp)\n",
    "\n",
    "ww = np.ones((3,3,5,7))\n",
    "cc = (3*3*7)*np.ones((6, 10, 5))\n",
    "assert close_enough(conv(aa, ww, stride=1), cc)\n",
    "\n",
    "# orientation test\n",
    "bb = np.array([1*np.eye(4), 3*np.eye(4)]) # 2 x 4 x 4\n",
    "\"\"\"\n",
    "    bb == [\n",
    "        [[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]],\n",
    "        [[3,0,0,0], [0,3,0,0], [0,0,3,0], [0,0,0,3]],\n",
    "        ]\n",
    "\"\"\"\n",
    "pp = np.array([[[1,1,0,0,], [0,0,1,1]]])\n",
    "assert close_enough(avgpool2x2(bb), pp)\n",
    "ww = np.zeros((2,2,1,4))\n",
    "ww[0,0,:,:] = 1 + np.arange(4)\n",
    "cc = np.array([1,2,3])[np.newaxis, :, np.newaxis] # shape 1 x 3 x 1\n",
    "assert close_enough(conv(bb, ww, stride=1), cc)\n",
    "\n",
    "print(\"hooray!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Derivatives ------------------------------------------------\n",
    "\n",
    "''' Why do we want dconv(x, w)/dw? So and only so that we can compute\n",
    "    dl/dw from dl/dconv(x,w). We ease our lives by writing a function that directly\n",
    "    gives dl/dw from dl/dconv(x,w).\n",
    "'''\n",
    "\n",
    "def Dw_conv(x, weights_shape, dl_dconv, stride=1):\n",
    "    H, W, C = x.shape\n",
    "    KH, KW, OD, ID = weights_shape\n",
    "    assert C == ID\n",
    "    HH, WW = int((H-KH+1)/stride), int((W-KW+1)/stride)\n",
    "    assert dl_dconv.shape == (HH, WW, OD)\n",
    "    # return an array of shape HH x WW x OD x ID\n",
    "    HS, WS = HH*stride, WW*stride\n",
    "    dl_dw = np.array(\n",
    "                [[np.tensordot(  \n",
    "                    dl_dconv                            , #HH x WW x OD\n",
    "                    x[dh:dh + HS:stride,dw:dw+WS:stride], # HH x WW x ID\n",
    "                    ((0,1), (0,1))\n",
    "                    )\n",
    "                    for dw in range(KW)]\n",
    "                 for dh in range(KH)]\n",
    "                )\n",
    "    return dl_dw\n",
    "\n",
    "''' Why do we want dconv(x, w)/dx? So and only so that we can compute\n",
    "    dl/dx from dl/dconv(x,w). We ease our lives by writing a function that directly\n",
    "    gives dl/dx from dl/dconv(x,w).\n",
    "'''\n",
    "\n",
    "def Dx_conv(x_shape, weights, dl_dconv, stride):\n",
    "    H, W, C = x_shape\n",
    "    KH, KW, OD, ID = weights.shape\n",
    "    assert C == ID\n",
    "    HH, WW = int((H-KH+1)/stride), int((W-KW+1)/stride)\n",
    "    # return H, W, ID\n",
    "    dl_dx = np.zeros((H,W,ID), dtype=np.float32)\n",
    "    for h in range(KH):\n",
    "        for w in range(KW):\n",
    "            dl_dx[h:h+HH*stride:stride, w:w + WW*stride:stride] += (\n",
    "                np.tensordot(\n",
    "                    dl_dconv,           # HHxWWxDD\n",
    "                    weights[h,w],       # ODxID\n",
    "                    ((2,),(0,))\n",
    "                    )\n",
    "            )\n",
    "            \n",
    "    return dl_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# ====== Backward Pass ======================================================\n",
    "\n",
    "def conv_backprop(abc, x, y):\n",
    "    A, B, C = abc\n",
    "    \n",
    "    h0 = avgpool2x2(x[:,:,np.newaxis])\n",
    "    #\n",
    "    z1 = conv(h0, C, stride=2)\n",
    "    #\n",
    "    h1 = lrelu(z1)\n",
    "    #\n",
    "    z2 = conv(h1, B, stride=1)\n",
    "    #\n",
    "    h2 = lrelu(z2)\n",
    "    #\n",
    "    z3 = A.dot(h2.flatten())\n",
    "    #\n",
    "    p = sigmoid(z3)\n",
    "\n",
    "    dl_dz3 = p - (1 if y==DIG_B else 0)\n",
    "    dl_dh2 = dl_dz3 * A.reshape(h2.shape)\n",
    "    dl_dz2 = dl_dh2 * dlrelu_dz(z2)\n",
    "    dl_dh1 = Dx_conv(h1.shape, B, dl_dz2, stride=1)\n",
    "    dl_dz1 = dl_dh1 * dlrelu_dz(z1)\n",
    "    # ATTN : figure out what \"*\" ought to mean\n",
    "\n",
    "    dl_dA = dl_dz3 * h2.flatten()\n",
    "    dl_dB = Dw_conv(h1, B.shape, dl_dz2, stride=1)\n",
    "    dl_dC = Dw_conv(h0, C.shape, dl_dz1, stride=2)\n",
    "    \n",
    "    return (dl_dA, dl_dB, dl_dC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "linear\n",
      "\n",
      "\n",
      "\n",
      "at step      0 tr acc 0.49 loss 0.853 te acc 0.49 loss 0.738\n",
      "at step   1000 tr acc 0.84 loss 0.555 te acc 0.92 loss 0.374\n",
      "at step   2000 tr acc 0.93 loss 0.201 te acc 0.92 loss 0.433\n",
      "at step   3000 tr acc 0.95 loss 0.128 te acc 0.93 loss 0.285\n",
      "at step   4000 tr acc 0.98 loss 0.046 te acc 0.93 loss 0.334\n",
      "at step   5000 tr acc 1.00 loss 0.017 te acc 0.93 loss 0.330\n",
      "at step   6000 tr acc 1.00 loss 0.014 te acc 0.93 loss 0.326\n",
      "at step   7000 tr acc 1.00 loss 0.009 te acc 0.93 loss 0.328\n",
      "at step   8000 tr acc 1.00 loss 0.008 te acc 0.93 loss 0.332\n",
      "at step   9000 tr acc 1.00 loss 0.008 te acc 0.93 loss 0.332\n",
      "at step  10000 tr acc 1.00 loss 0.008 te acc 0.93 loss 0.330\n",
      "at step  11000 tr acc 1.00 loss 0.007 te acc 0.93 loss 0.334\n",
      "at step  12000 tr acc 1.00 loss 0.007 te acc 0.93 loss 0.332\n",
      "at step  13000 tr acc 1.00 loss 0.007 te acc 0.93 loss 0.331\n",
      "at step  14000 tr acc 1.00 loss 0.007 te acc 0.93 loss 0.333\n",
      "at step  15000 tr acc 1.00 loss 0.007 te acc 0.93 loss 0.334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 178132.34it/s]\n",
      "100%|██████████| 1991/1991 [00:00<00:00, 123529.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after all training tr acc 1.00 loss 0.007 te acc 0.93 loss 0.336\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "vanilla\n",
      "\n",
      "\n",
      "\n",
      "at step      0 tr acc 0.50 loss 0.697 te acc 0.51 loss 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at step   1000 tr acc 0.65 loss 0.685 te acc 0.62 loss 0.679\n",
      "at step   2000 tr acc 0.65 loss 0.595 te acc 0.87 loss 0.498\n",
      "at step   3000 tr acc 0.91 loss 0.276 te acc 0.89 loss 0.302\n",
      "at step   4000 tr acc 0.93 loss 0.213 te acc 0.90 loss 0.245\n",
      "at step   5000 tr acc 0.95 loss 0.165 te acc 0.90 loss 0.299\n",
      "at step   6000 tr acc 0.96 loss 0.120 te acc 0.90 loss 0.324\n",
      "at step   7000 tr acc 0.99 loss 0.056 te acc 0.93 loss 0.220\n",
      "at step   8000 tr acc 1.00 loss 0.028 te acc 0.92 loss 0.276\n",
      "at step   9000 tr acc 0.99 loss 0.045 te acc 0.91 loss 0.339\n",
      "at step  10000 tr acc 0.99 loss 0.014 te acc 0.92 loss 0.361\n",
      "at step  11000 tr acc 1.00 loss 0.008 te acc 0.92 loss 0.370\n",
      "at step  12000 tr acc 1.00 loss 0.004 te acc 0.92 loss 0.398\n",
      "at step  13000 tr acc 1.00 loss 0.003 te acc 0.92 loss 0.399\n",
      "at step  14000 tr acc 1.00 loss 0.003 te acc 0.92 loss 0.410\n",
      "at step  15000 tr acc 1.00 loss 0.002 te acc 0.92 loss 0.420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 24461.29it/s]\n",
      "100%|██████████| 1991/1991 [00:00<00:00, 22160.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after all training tr acc 1.00 loss 0.002 te acc 0.92 loss 0.400\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "conv\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at step      0 tr acc 0.50 loss 0.693 te acc 0.51 loss 0.690\n",
      "at step   1000 tr acc 0.74 loss 0.557 te acc 0.79 loss 0.410\n",
      "at step   2000 tr acc 0.78 loss 0.458 te acc 0.91 loss 0.224\n",
      "at step   3000 tr acc 0.76 loss 0.491 te acc 0.93 loss 0.215\n",
      "at step   4000 tr acc 0.79 loss 0.442 te acc 0.94 loss 0.185\n",
      "at step   5000 tr acc 0.79 loss 0.462 te acc 0.93 loss 0.224\n",
      "at step   6000 tr acc 0.78 loss 0.492 te acc 0.92 loss 0.214\n",
      "at step   7000 tr acc 0.81 loss 0.430 te acc 0.93 loss 0.180\n",
      "at step   8000 tr acc 0.80 loss 0.429 te acc 0.94 loss 0.160\n",
      "at step   9000 tr acc 0.80 loss 0.418 te acc 0.94 loss 0.156\n",
      "at step  10000 tr acc 0.82 loss 0.405 te acc 0.94 loss 0.156\n",
      "at step  11000 tr acc 0.81 loss 0.401 te acc 0.95 loss 0.151\n",
      "at step  12000 tr acc 0.81 loss 0.401 te acc 0.95 loss 0.152\n",
      "at step  13000 tr acc 0.81 loss 0.400 te acc 0.94 loss 0.156\n",
      "at step  14000 tr acc 0.83 loss 0.393 te acc 0.94 loss 0.155\n",
      "at step  15000 tr acc 0.83 loss 0.382 te acc 0.95 loss 0.151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1481.73it/s]\n",
      "100%|██████████| 1991/1991 [00:01<00:00, 1301.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after all training tr acc 0.83 loss 0.382 te acc 0.95 loss 0.150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# ======= TRAINING LOOP ==================================================\n",
    "\n",
    "# ========================================================================\n",
    "# ======= Training Parameters ==================================================\n",
    "T = 15001\n",
    "DT = 1000\n",
    "LEARNING_RATE = 0.01\n",
    "ANNEAL_T = 4000\n",
    "DRAG_COEF = 0.1\n",
    "\n",
    "idx = 0\n",
    "def next_training_example():\n",
    "    global idx, x_train, y_train\n",
    "    xy = x_train[idx], y_train[idx]\n",
    "    idx += 1\n",
    "    if idx==N:\n",
    "        idx = 0\n",
    "        x_train, y_train = shuffle(x_train, y_train)\n",
    "    return xy\n",
    "\n",
    "#-------- Interface with Model -------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "FUNCS_BY_MODEL =    {\n",
    "        \"linear\"    :(linear_init, linear_backprop, linear_displace, linear_predict),\n",
    "        \"vanilla\"   :(vanilla_init, vanilla_backprop, vanilla_displace, vanilla_predict),\n",
    "        \"conv\"      :(conv_init, conv_backprop, conv_displace, conv_predict)\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# ========================================================================\n",
    "# ======= SGD: the Engine of Learning ====================================\n",
    "\n",
    "for MODEL in (\"linear\", \"vanilla\", \"conv\"):\n",
    "    print(\"\\n\"*4)\n",
    "    print(MODEL)\n",
    "    print(\"\\n\"*2)\n",
    "    INIT, BACK, DISP, PRED = FUNCS_BY_MODEL[MODEL]\n",
    "\n",
    "    w = INIT()\n",
    "    m = DISP(w, -1., w) # hacky way to set m=0 of same shape as w\n",
    "    for t in range(T):\n",
    "        x, y = next_training_example()\n",
    "        g = BACK(w, x, y)\n",
    "        LR = LEARNING_RATE * float(ANNEAL_T) / (ANNEAL_T + t)\n",
    "        m = DISP(m, -DRAG_COEF, m) # m forgets a bit of its past\n",
    "        m = DISP(m, +1., g) # add gradient to momentum\n",
    "        w = DISP(w, -LR, m) # update based on momentum\n",
    "\n",
    "        if t%DT : continue\n",
    "\n",
    "        xs = x_train[-1000:]\n",
    "        ys = y_train[-1000:]\n",
    "        mstr = judge(lambda x: PRED(w, x), xs, ys)\n",
    "        xs = x_test[-1000:]\n",
    "        ys = y_test[-1000:]\n",
    "        mste = judge(lambda x: PRED(w, x), xs, ys)\n",
    "        print(\"at step {:6d}\".format(t),\n",
    "            \"tr acc {:4.2f} loss {:5.3f}\".format(mstr[\"acc\"], mstr[\"loss\"]),\n",
    "            \"te acc {:4.2f} loss {:5.3f}\".format(mste[\"acc\"], mste[\"loss\"])\n",
    "            )\n",
    "        \n",
    "    xs = x_train[:]\n",
    "    ys = y_train[:]\n",
    "    mstr = judge(lambda x: PRED(w, x), xs, ys, verbose=True)\n",
    "    xs = x_test[:]\n",
    "    ys = y_test[:]\n",
    "    mste = judge(lambda x: PRED(w, x), xs, ys, verbose=True)\n",
    "    print(\"after all training\",\n",
    "            \"tr acc {:4.2f} loss {:5.3f}\".format(mstr[\"acc\"], mstr[\"loss\"]),\n",
    "            \"te acc {:4.2f} loss {:5.3f}\".format(mste[\"acc\"], mste[\"loss\"])\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST DATA PERFORMANCE\n",
    "\n",
    "\n",
    "#### N ~ 12000\n",
    "\n",
    "Linear\n",
    "acc: 0.97      loss: 0.93   \n",
    "\n",
    "Vanilla\n",
    "acc: 0.98      loss: 0.047\n",
    "\n",
    "Convolution\n",
    "acc: 0.98      loss: 0.045\n",
    "\n",
    "\n",
    "#### N = 1000 with noise\n",
    "\n",
    "Linear\n",
    "acc: 0.93      loss: 0.336   \n",
    "\n",
    "Vanilla\n",
    "acc: 0.92      loss: 0.400\n",
    "\n",
    "Convolution\n",
    "acc: 0.95      loss: 0.150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------\n",
    "### CONCLUSION:\n",
    "conv nets better generalize on this image domain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6.86x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
